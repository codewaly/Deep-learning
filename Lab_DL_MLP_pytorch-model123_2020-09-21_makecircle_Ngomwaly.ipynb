{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4fnzJJDo60Y"
   },
   "source": [
    "# Lab Deep Learning / Multi-Layer Perceptron for binary-classification / in pytorch\n",
    "\n",
    "**Author: geoffroy.peeters@telecom-paris.fr**\n",
    "\n",
    "**Version**: 2020/09/21\n",
    "\n",
    "For any remark or suggestion, please feel free to contact me.\n",
    "\n",
    "\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The objective of this lab is to develop a two hidden layers MLP to perform **binary classification**.\n",
    "\n",
    "We will use a MLP with 2 hidden layer with $n_{h1}=20$ and $n_{h2}=10$ hidden units and ```relu``` activation functions.\n",
    "You will perform 10000 iterations (epochs) of SGD to find the parameters.\n",
    "\n",
    "Note: for this lab, we do not separate the dataset into a train, validation and test part.\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "You should normalize the data to zero mean and unit standard deviation\n",
    "\n",
    "### Model\n",
    "\n",
    "There are various ways to write NN model in pytorch. \n",
    "\n",
    "In this lab, you will write three different implementations:\n",
    "- **Model A**: manually defining the parameters (W1,b1,W2,b2,W3,b3), writing the forward equations, writting the loss equation, calling the .backward() and manually updating the weights using W1.grad. You will write the loop to perform 1000 epochs.\n",
    "- **Model B**: using the Sequential class of pytorch\n",
    "- **Model C**: a custom torch.nn.Module class for this.\n",
    "\n",
    "For Model B and C, you will use the ready made loss and optimization from the nn and optim packages. You can use the same code to optimize the parameters of Model B and C.\n",
    "\n",
    "### Loss\n",
    "\n",
    "Since we are dealing with a binary classification problem, we will use a Binary Cross Entropy loss (use ```torch.nn.BCELoss``` for Model B and C).\n",
    "\n",
    "### Parameters update/ Optimization\n",
    "\n",
    "For updating the parameters, we will use as optimizer a simple SGD algorithm (use ```torch.optim.SGD``` for Model B and C) with a learning rate of 0.1.\n",
    "\n",
    "Don't forget that an optimizer is applied to a set of parameters (```my_model.parameters()``` gives the parameters of the network for Model B and C).\n",
    "Once the gradients have been computed (after the backpropagation has been performed), you can perform one step of optimization (using ```optimizer.step()``` for Model B and C).\n",
    "\n",
    "### Backward propagation\n",
    "\n",
    "Backpropagation is automatically performed in pytorch using the ```autograd``` package. \n",
    "First, reset the gradients of all parameters (using ```optimizer.zero_grad()``` for Model B and C), then perform the backpropagation ```loss.backward()```. \n",
    "\n",
    "## Your task:\n",
    "\n",
    "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
    "\n",
    "## Documentation:\n",
    "- NN: https://pytorch.org/docs/stable/nn.html\n",
    "- Autograd: https://pytorch.org/docs/stable/autograd.html\n",
    "- Optim: https://pytorch.org/docs/stable/optim.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuvU8y2Lo60Z"
   },
   "source": [
    "## Load the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1VTuwVio60a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "student = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zof__thjo60d",
    "outputId": "09947749-2415-493d-d746-050bf0670ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bsb-phrJo60g"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We take the usual circle dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otam7ukPo60g"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X_np, y_np = datasets.make_circles(n_samples=1000, noise=0.2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5idAV4Co60i"
   },
   "source": [
    "We convert the ```numpy tensors``` to ```torch tensors```. \n",
    "The difference being that the latters allows to do automatic gradient differentiation (back-propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPxnzVSDo60j"
   },
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_np).float()\n",
    "y = torch.from_numpy(y_np).float()\n",
    "y = y.view(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YHgd8JYPo60l",
    "outputId": "d273d301-ebb5-448e-9084-08b8e0f73f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 1])\n",
      "tensor([ 0.0030, -0.0087])\n",
      "tensor([0.5886, 0.6050])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Unp-3kjjo60n"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6rIwFaauo60n",
    "outputId": "c408850f-62b6-4459-c6df-5350fac5f4b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0490e-08, 9.5367e-09])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "X -= X.mean(dim=0)\n",
    "X /= X.std(dim=0)\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rc3VgVWOo60p"
   },
   "source": [
    "## Definition of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrBQMqCJo60r"
   },
   "outputs": [],
   "source": [
    "n_in = X.shape[1]\n",
    "n_h1 = 20\n",
    "n_h2 = 10\n",
    "n_out = 1\n",
    "\n",
    "nb_epoch = 10000\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIzjntgZo60t"
   },
   "source": [
    "## Model 1 (writing the network equations)\n",
    "\n",
    "Here, you will define the variables and write the equations of the network yourself (as you would do in numpy).\n",
    "However you will use ```torch tensors``` instead of ```numpy array```. \n",
    "\n",
    "***Why ?*** because torch tensors will allows you to automatically get the gradient. You will use ```loss.backward``` to launch the backpropagation from ```loss```. Then, for all tensors you created and for which you declared ```requires_grad=True```, you will get the gradient of ```loss```with respect to this variable in the field ```.grad```. \n",
    "\n",
    "***Example*** ```W1 = torch.Tensor(..., requires_grad=True)``` ... ```loss.backward``` will have the gradient $\\frac{d Loss}{d W1}$in ```W1.grad```.\n",
    "\n",
    "Don't forget that the weight $W_1, W_2, \\cdots$ matrices should be initialized randomly with small values; while the bias vectors $b_1, b_2, \\cdots$can be initialized to zero.\n",
    "\n",
    "***Important: pytorch restrication***\n",
    "When you update the parameters (W1, b1, ...) but substracting a small part of the gradient, you should that ``inplace`` (W1 -= ??? and not W1 = W1 - ???). Otherwise pytorch will overide W1 by its value (it will eras its .grad filed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "HB65rqW4o60u",
    "outputId": "32f47a63-c93d-492b-ebca-473a896697cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.6931491494178772\n",
      "epoch 500, loss 0.6931469440460205\n",
      "epoch 1000, loss 0.6931465864181519\n",
      "epoch 1500, loss 0.6931461691856384\n",
      "epoch 2000, loss 0.6931454539299011\n",
      "epoch 2500, loss 0.6931445598602295\n",
      "epoch 3000, loss 0.6931431889533997\n",
      "epoch 3500, loss 0.6931408047676086\n",
      "epoch 4000, loss 0.6931366324424744\n",
      "epoch 4500, loss 0.6931281089782715\n",
      "epoch 5000, loss 0.693108081817627\n",
      "epoch 5500, loss 0.6930463314056396\n",
      "epoch 6000, loss 0.6927465200424194\n",
      "epoch 6500, loss 0.6882805228233337\n",
      "epoch 7000, loss 0.4968372583389282\n",
      "epoch 7500, loss 0.2827962040901184\n",
      "epoch 8000, loss 0.27359554171562195\n",
      "epoch 8500, loss 0.27059900760650635\n",
      "epoch 9000, loss 0.2690599858760834\n",
      "epoch 9500, loss 0.2683098316192627\n",
      "epoch 10000, loss 0.2678014934062958\n",
      "epoch 10500, loss 0.2673732340335846\n",
      "epoch 11000, loss 0.2671282887458801\n",
      "epoch 11500, loss 0.26694533228874207\n",
      "epoch 12000, loss 0.26679980754852295\n",
      "epoch 12500, loss 0.26667824387550354\n",
      "epoch 13000, loss 0.2665739953517914\n",
      "epoch 13500, loss 0.2664870619773865\n",
      "epoch 14000, loss 0.2664134204387665\n",
      "epoch 14500, loss 0.2663505971431732\n",
      "epoch 15000, loss 0.2662956416606903\n",
      "epoch 15500, loss 0.2662476599216461\n",
      "epoch 16000, loss 0.26620349287986755\n",
      "epoch 16500, loss 0.266165167093277\n",
      "epoch 17000, loss 0.26612693071365356\n",
      "epoch 17500, loss 0.2660958468914032\n",
      "epoch 18000, loss 0.26606762409210205\n",
      "epoch 18500, loss 0.26604074239730835\n",
      "epoch 19000, loss 0.2660175859928131\n",
      "epoch 19500, loss 0.26599469780921936\n",
      "epoch 20000, loss 0.2659739553928375\n",
      "epoch 20500, loss 0.26595526933670044\n",
      "epoch 21000, loss 0.2659382224082947\n",
      "epoch 21500, loss 0.2659216523170471\n",
      "epoch 22000, loss 0.2659052014350891\n",
      "epoch 22500, loss 0.2658899128437042\n",
      "epoch 23000, loss 0.26587536931037903\n",
      "epoch 23500, loss 0.265860915184021\n",
      "epoch 24000, loss 0.2658466100692749\n",
      "epoch 24500, loss 0.2658328115940094\n",
      "epoch 25000, loss 0.2658189833164215\n",
      "epoch 25500, loss 0.26580485701560974\n",
      "epoch 26000, loss 0.26579052209854126\n",
      "epoch 26500, loss 0.26577645540237427\n",
      "epoch 27000, loss 0.26576200127601624\n",
      "epoch 27500, loss 0.26574593782424927\n",
      "epoch 28000, loss 0.26573002338409424\n",
      "epoch 28500, loss 0.2657119929790497\n",
      "epoch 29000, loss 0.2656942307949066\n",
      "epoch 29500, loss 0.26567307114601135\n",
      "epoch 30000, loss 0.2656517028808594\n",
      "epoch 30500, loss 0.2656269669532776\n",
      "epoch 31000, loss 0.2656005024909973\n",
      "epoch 31500, loss 0.26556915044784546\n",
      "epoch 32000, loss 0.2655360698699951\n",
      "epoch 32500, loss 0.26549819111824036\n",
      "epoch 33000, loss 0.2654547393321991\n",
      "epoch 33500, loss 0.2654072046279907\n",
      "epoch 34000, loss 0.26535260677337646\n",
      "epoch 34500, loss 0.265290766954422\n",
      "epoch 35000, loss 0.26522132754325867\n",
      "epoch 35500, loss 0.26514261960983276\n",
      "epoch 36000, loss 0.2650538384914398\n",
      "epoch 36500, loss 0.2649533748626709\n",
      "epoch 37000, loss 0.2648419439792633\n",
      "epoch 37500, loss 0.2647160589694977\n",
      "epoch 38000, loss 0.26457494497299194\n",
      "epoch 38500, loss 0.2644181549549103\n",
      "epoch 39000, loss 0.2642434239387512\n",
      "epoch 39500, loss 0.26405131816864014\n",
      "epoch 40000, loss 0.2638383209705353\n",
      "epoch 40500, loss 0.26360219717025757\n",
      "epoch 41000, loss 0.26334381103515625\n",
      "epoch 41500, loss 0.2630766034126282\n",
      "epoch 42000, loss 0.2627968192100525\n",
      "epoch 42500, loss 0.262508362531662\n",
      "epoch 43000, loss 0.26222139596939087\n",
      "epoch 43500, loss 0.26188918948173523\n",
      "epoch 44000, loss 0.26158979535102844\n",
      "epoch 44500, loss 0.26126745343208313\n",
      "epoch 45000, loss 0.26090556383132935\n",
      "epoch 45500, loss 0.2606232464313507\n",
      "epoch 46000, loss 0.26023706793785095\n",
      "epoch 46500, loss 0.25829267501831055\n",
      "epoch 47000, loss 0.2574787437915802\n",
      "epoch 47500, loss 0.25695785880088806\n",
      "epoch 48000, loss 0.25648510456085205\n",
      "epoch 48500, loss 0.25613951683044434\n",
      "epoch 49000, loss 0.2559192478656769\n",
      "epoch 49500, loss 0.2557666003704071\n",
      "epoch 50000, loss 0.2556600570678711\n",
      "epoch 50500, loss 0.25557851791381836\n",
      "epoch 51000, loss 0.2555113434791565\n",
      "epoch 51500, loss 0.25545382499694824\n",
      "epoch 52000, loss 0.2553726136684418\n",
      "epoch 52500, loss 0.2553201913833618\n",
      "epoch 53000, loss 0.2552724778652191\n",
      "epoch 53500, loss 0.25522708892822266\n",
      "epoch 54000, loss 0.25518181920051575\n",
      "epoch 54500, loss 0.2551356852054596\n",
      "epoch 55000, loss 0.25508758425712585\n",
      "epoch 55500, loss 0.2550375759601593\n",
      "epoch 56000, loss 0.25498467683792114\n",
      "epoch 56500, loss 0.2549302875995636\n",
      "epoch 57000, loss 0.25487232208251953\n",
      "epoch 57500, loss 0.25480949878692627\n",
      "epoch 58000, loss 0.25474196672439575\n",
      "epoch 58500, loss 0.2546643316745758\n",
      "epoch 59000, loss 0.25457271933555603\n",
      "epoch 59500, loss 0.25447696447372437\n",
      "epoch 60000, loss 0.2543775737285614\n",
      "epoch 60500, loss 0.25427520275115967\n",
      "epoch 61000, loss 0.25416722893714905\n",
      "epoch 61500, loss 0.25405263900756836\n",
      "epoch 62000, loss 0.2539323568344116\n",
      "epoch 62500, loss 0.2538045048713684\n",
      "epoch 63000, loss 0.2536717355251312\n",
      "epoch 63500, loss 0.253528356552124\n",
      "epoch 64000, loss 0.2533845901489258\n",
      "epoch 64500, loss 0.25323644280433655\n",
      "epoch 65000, loss 0.2530865967273712\n",
      "epoch 65500, loss 0.25293487310409546\n",
      "epoch 66000, loss 0.25277966260910034\n",
      "epoch 66500, loss 0.2526210844516754\n",
      "epoch 67000, loss 0.25246113538742065\n",
      "epoch 67500, loss 0.25229766964912415\n",
      "epoch 68000, loss 0.25213295221328735\n",
      "epoch 68500, loss 0.2519671618938446\n",
      "epoch 69000, loss 0.25180086493492126\n",
      "epoch 69500, loss 0.25163576006889343\n",
      "epoch 70000, loss 0.2514744699001312\n",
      "epoch 70500, loss 0.2512955367565155\n",
      "epoch 71000, loss 0.25111639499664307\n",
      "epoch 71500, loss 0.25094714760780334\n",
      "epoch 72000, loss 0.25078389048576355\n",
      "epoch 72500, loss 0.25062787532806396\n",
      "epoch 73000, loss 0.2504655122756958\n",
      "epoch 73500, loss 0.2503139078617096\n",
      "epoch 74000, loss 0.2501577138900757\n",
      "epoch 74500, loss 0.24999462068080902\n",
      "epoch 75000, loss 0.24984735250473022\n",
      "epoch 75500, loss 0.24969333410263062\n",
      "epoch 76000, loss 0.24953658878803253\n",
      "epoch 76500, loss 0.24934761226177216\n",
      "epoch 77000, loss 0.2491798996925354\n",
      "epoch 77500, loss 0.2490084171295166\n",
      "epoch 78000, loss 0.24879054725170135\n",
      "epoch 78500, loss 0.24861742556095123\n",
      "epoch 79000, loss 0.24838711321353912\n",
      "epoch 79500, loss 0.24820886552333832\n",
      "epoch 80000, loss 0.24801433086395264\n",
      "epoch 80500, loss 0.24786080420017242\n",
      "epoch 81000, loss 0.2477177083492279\n",
      "epoch 81500, loss 0.24757932126522064\n",
      "epoch 82000, loss 0.24744747579097748\n",
      "epoch 82500, loss 0.24727195501327515\n",
      "epoch 83000, loss 0.24710986018180847\n",
      "epoch 83500, loss 0.24695099890232086\n",
      "epoch 84000, loss 0.24679331481456757\n",
      "epoch 84500, loss 0.24645674228668213\n",
      "epoch 85000, loss 0.24624091386795044\n",
      "epoch 85500, loss 0.24602165818214417\n",
      "epoch 86000, loss 0.24581855535507202\n",
      "epoch 86500, loss 0.2456226795911789\n",
      "epoch 87000, loss 0.24542973935604095\n",
      "epoch 87500, loss 0.24524202942848206\n",
      "epoch 88000, loss 0.24506403505802155\n",
      "epoch 88500, loss 0.24489401280879974\n",
      "epoch 89000, loss 0.24473516643047333\n",
      "epoch 89500, loss 0.24456973373889923\n",
      "epoch 90000, loss 0.24442549049854279\n",
      "epoch 90500, loss 0.2442721575498581\n",
      "epoch 91000, loss 0.24414032697677612\n",
      "epoch 91500, loss 0.24402263760566711\n",
      "epoch 92000, loss 0.2438892275094986\n",
      "epoch 92500, loss 0.24377954006195068\n",
      "epoch 93000, loss 0.24366562068462372\n",
      "epoch 93500, loss 0.24353979527950287\n",
      "epoch 94000, loss 0.24341455101966858\n",
      "epoch 94500, loss 0.24329997599124908\n",
      "epoch 95000, loss 0.24316254258155823\n",
      "epoch 95500, loss 0.24307605624198914\n",
      "epoch 96000, loss 0.24295413494110107\n",
      "epoch 96500, loss 0.24285054206848145\n",
      "epoch 97000, loss 0.24275149405002594\n",
      "epoch 97500, loss 0.2426442801952362\n",
      "epoch 98000, loss 0.24253785610198975\n",
      "epoch 98500, loss 0.24210050702095032\n",
      "epoch 99000, loss 0.2415081411600113\n",
      "epoch 99500, loss 0.24135178327560425\n"
     ]
    }
   ],
   "source": [
    "# --- We first initialize the variables of the network (W1, b1, ...)\n",
    "if student:\n",
    "    # --- START CODE HERE (01)\n",
    "    W1 = torch.rand(n_in,n_h1)*0.01\n",
    "    W1.requires_grad = True\n",
    "    b1 = torch.zeros(n_h1)\n",
    "    b1.requires_grad = True \n",
    "    \n",
    "    W2 = torch.rand(n_h1,n_h2)*0.01\n",
    "    W2.requires_grad = True\n",
    "    b2 = torch.zeros(n_h2)\n",
    "    b2.requires_grad = True \n",
    "\n",
    "    W3 = torch.rand(n_h2,1)*0.01\n",
    "    W3.requires_grad = True\n",
    "    b3 =torch.zeros(1)\n",
    "    b3.requires_grad = True  \n",
    "    # --- END CODE HERE\n",
    "\n",
    "\n",
    "# --- We then write a function to perform the forward pass (using pytorch opertaors, not numpy operators)\n",
    "# --- taking X as input and returing hat_y as output\n",
    "relu = nn.ReLU()    \n",
    "def model(X):\n",
    "    if student:\n",
    "        # --- START CODE HERE (02)\n",
    "        A0 = X    \n",
    "        Z1 = torch.mm(A0,W1) +b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = torch.mm(A1,W2) +b2\n",
    "        A2 = relu(Z2)\n",
    "        Z3 = torch.mm(A2,W3) + b3\n",
    "        A3 = torch.sigmoid(Z3)\n",
    "        hat_y = A3\n",
    "        # --- END CODE HERE\n",
    "    return hat_y\n",
    "\n",
    "# --- We then iterate over epochs (we do not perform split into mini-batch here)\n",
    "# --- For each iteration, we\n",
    "# ---   a) perform the forward pass, \n",
    "# ---   b) compute the loss/cost, \n",
    "# ---   c) compute the backward pass to get the gradients of the cost w.r.t. the parameters W1, b1, ...\n",
    "# ---   d) perform the update of the parameters W1, b1, ...\n",
    "for num_epoch in range(0, nb_epoch):    \n",
    "\n",
    "    # --- a) Forward pass: X (n_in, N), hat_y (n_out, N)\n",
    "    hat_y = model(X)\n",
    "\n",
    "    # -- We clip hat_y in order to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    hat_y = torch.clamp(hat_y, eps, 1-eps)\n",
    "    \n",
    "    # --- b) Computing the loss/cost\n",
    "    if student:\n",
    "        # --- START CODE HERE (03)\n",
    "        loss = -(y*torch.log(hat_y)+(1-y)*torch.log(1-hat_y))    #torch.nn.BCELoss()\n",
    "        cost = torch.mean(loss) \n",
    "        # --- END CODE HERE\n",
    "    \n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, cost))\n",
    "\n",
    "    # --- c) Backward pass\n",
    "    cost.backward()\n",
    "    \n",
    "    # --- \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
    "    with torch.no_grad():\n",
    "        # --- d) perform the update of the parameters W1, b1, ...\n",
    "        if student:\n",
    "            # --- the gradients dLoss/dW1 is stored in W1.grad, dLoss/db1 is stored in b1.grad, ...\n",
    "            # --- START CODE HERE (04)\n",
    "            W1 -=  alpha*W1.grad\n",
    "            b1 -=  alpha*b1.grad\n",
    "            W2 -=  alpha*W2.grad\n",
    "            b2 -=  alpha*b2.grad\n",
    "            W3 -=  alpha*W3.grad\n",
    "            b3 -=  alpha*b3.grad\n",
    "            # --- END CODE HERE\n",
    "\n",
    "    # --- We need to set to zero all gradients (otherwise they are cumulated)\n",
    "    W1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    W2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "    W3.grad.zero_()\n",
    "    b3.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxgA4DXVo60w"
   },
   "source": [
    "## Model 2 (using nn.sequential)\n",
    "\n",
    "Here, you will use the package ```torch.nn``` which comes with a predefined set of layers. The syntax is close to the one of ```keras```(```Sequential```), but differs in the fact that layers are splitted into the matrix multiplication followed by a non-linear activations (```keras```merge both using the ```Dense```layers).\n",
    "\n",
    "The model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. It is therefore a convenient way to write simple sequential networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elGQpQzjo60x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=20, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if student:\n",
    "    # --- START CODE HERE (05)\n",
    "    my_model =nn.Sequential(nn.Linear(n_in,n_h1),nn.ReLU(),nn.Linear(n_h1,n_h2),nn.ReLU(),\n",
    "                            nn.Linear(n_h2,n_out),nn.Sigmoid())\n",
    "    # --- END CODE HERE\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4twpnbEAo60z"
   },
   "source": [
    "## Model 3 (using a class definition)\n",
    "\n",
    "Here, you will write the network using the recommended pytroch way; i.e. by defining a class.\n",
    "This class inherit from the main class ```torch.nn.Module```.\n",
    "You only need to write the ```__init__``` method and the ```forward``` method.\n",
    "\n",
    "In object programming, the ```__init__``` method defines the attributes of your class. Since the attributes of your  network are the parameters to be trained (weights and biases), you should declare in the ```__init``` all the layers that involve parameters to be trained (mostly the ```Linear```layers which perform the matrix multiplication).\n",
    "\n",
    "The ```forward``` method contains the code of the forward pass itself. It can of course call attributes defined in the ```__init___``` method. It is the method used when calling ```model(x)```.\n",
    "\n",
    "As before, the model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. \n",
    "\n",
    "Classes are convenient way to write more complex network than what you can do with ```nn.sequential```. Note that you can actually include a ```nn.sequential``` in your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNsNkq9Do60z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=2, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (06)\n",
    "            self.fc1 = nn.Linear(n_in,n_h1) # hidden layer 1\n",
    "            self.fc2 = nn.Linear(n_h1,n_h2) # hidden layer 2\n",
    "            self.fc3 = nn.Linear(n_h2,n_out)  # output layer\n",
    "            # --- END CODE HERE\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (07)\n",
    "            A0 = X\n",
    "            A1 = F.relu(self.fc1(A0))   # activation function for hidden layer 1\n",
    "            A2 = F.relu(self.fc2(A1))   # activation function for hidden layer 2\n",
    "            A3 = torch.sigmoid(self.fc3(A2))   # activation function for output layer\n",
    "            # --- END CODE HERE\n",
    "\n",
    "        return A3\n",
    "\n",
    "# --- START CODE HERE\n",
    "my_model = Net(n_in, n_h1, n_h2, n_out)\n",
    "# --- END CODE HERE\n",
    "my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ts4aVeIPo601"
   },
   "source": [
    "## Criterion and Optimization for model 2 and model 3\n",
    "\n",
    "The code of Model 1 is self-contained, i.e. it already contains all necessary instruction to perform forawrd, loss, backward and parameter updates.\n",
    "\n",
    "When using ```nn.sequential``` (model 2) or a class definition of the network (model 3), we still need to define \n",
    "- what we will minimize (the loss to be minimized, i.e. Binary-Cross-Entropy). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
    "- how we will minimize the loss, i.e. what parameter update algorithms we will use (SGD, momentum). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs63V-Wgo602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    foreach: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if student:\n",
    "    # --- START CODE HERE (08)\n",
    "    criterion =nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(my_model.parameters(),lr=0.1)\n",
    "    # --- END CODE HERE\n",
    "criterion\n",
    "optimizer #torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qL7ePZi9o604"
   },
   "source": [
    "## Training for model 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XamuBM_ho604"
   },
   "source": [
    "Having defined the network, the citerion to be minimized and the optimizer, we then perform a loop over epochs (iterations); at each step we\n",
    "- compute the forward pass by passing the data to the model: ```haty = model(x)```\n",
    "- compute the the loss (the criterion)\n",
    "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
    "- computing the backpropagation (using as before ```.backward()```)\n",
    "- performing one step of optimization (using ```.step()```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "colab_type": "code",
    "id": "rKfrD8V3o605",
    "outputId": "407d92a0-aa42-4860-faf0-89f608045957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.706221878528595\n",
      "epoch 500, loss 0.2706892192363739\n",
      "epoch 1000, loss 0.2644668221473694\n",
      "epoch 1500, loss 0.26335883140563965\n",
      "epoch 2000, loss 0.26277366280555725\n",
      "epoch 2500, loss 0.2625350058078766\n",
      "epoch 3000, loss 0.2624008059501648\n",
      "epoch 3500, loss 0.2623209059238434\n",
      "epoch 4000, loss 0.2622687816619873\n",
      "epoch 4500, loss 0.26222556829452515\n",
      "epoch 5000, loss 0.2621804177761078\n",
      "epoch 5500, loss 0.2621508240699768\n",
      "epoch 6000, loss 0.2621253728866577\n",
      "epoch 6500, loss 0.2621019780635834\n",
      "epoch 7000, loss 0.2620812952518463\n",
      "epoch 7500, loss 0.2620624899864197\n",
      "epoch 8000, loss 0.262043833732605\n",
      "epoch 8500, loss 0.2620270848274231\n",
      "epoch 9000, loss 0.2620106637477875\n",
      "epoch 9500, loss 0.26199451088905334\n",
      "epoch 10000, loss 0.2619757056236267\n",
      "epoch 10500, loss 0.2619602680206299\n",
      "epoch 11000, loss 0.26194503903388977\n",
      "epoch 11500, loss 0.26192983984947205\n",
      "epoch 12000, loss 0.2619066536426544\n",
      "epoch 12500, loss 0.26188305020332336\n",
      "epoch 13000, loss 0.26178884506225586\n",
      "epoch 13500, loss 0.26160502433776855\n",
      "epoch 14000, loss 0.26146575808525085\n",
      "epoch 14500, loss 0.2613149881362915\n",
      "epoch 15000, loss 0.2611425518989563\n",
      "epoch 15500, loss 0.26098310947418213\n",
      "epoch 16000, loss 0.2608417868614197\n",
      "epoch 16500, loss 0.26070669293403625\n",
      "epoch 17000, loss 0.2605739235877991\n",
      "epoch 17500, loss 0.26043596863746643\n",
      "epoch 18000, loss 0.2603071630001068\n",
      "epoch 18500, loss 0.2601521909236908\n",
      "epoch 19000, loss 0.2600124776363373\n",
      "epoch 19500, loss 0.25989073514938354\n",
      "epoch 20000, loss 0.25978729128837585\n",
      "epoch 20500, loss 0.2596918046474457\n",
      "epoch 21000, loss 0.2596050500869751\n",
      "epoch 21500, loss 0.25952234864234924\n",
      "epoch 22000, loss 0.2594451904296875\n",
      "epoch 22500, loss 0.25937384366989136\n",
      "epoch 23000, loss 0.2593051493167877\n",
      "epoch 23500, loss 0.25923967361450195\n",
      "epoch 24000, loss 0.25917381048202515\n",
      "epoch 24500, loss 0.2591094970703125\n",
      "epoch 25000, loss 0.2590339481830597\n",
      "epoch 25500, loss 0.2589656710624695\n",
      "epoch 26000, loss 0.2589016556739807\n",
      "epoch 26500, loss 0.2588396370410919\n",
      "epoch 27000, loss 0.2587791979312897\n",
      "epoch 27500, loss 0.2587205171585083\n",
      "epoch 28000, loss 0.25865602493286133\n",
      "epoch 28500, loss 0.2585960626602173\n",
      "epoch 29000, loss 0.2585378587245941\n",
      "epoch 29500, loss 0.2584749758243561\n",
      "epoch 30000, loss 0.2583666741847992\n",
      "epoch 30500, loss 0.25812843441963196\n",
      "epoch 31000, loss 0.25805437564849854\n",
      "epoch 31500, loss 0.257985919713974\n",
      "epoch 32000, loss 0.2579229474067688\n",
      "epoch 32500, loss 0.25786519050598145\n",
      "epoch 33000, loss 0.257808655500412\n",
      "epoch 33500, loss 0.25738731026649475\n",
      "epoch 34000, loss 0.25729772448539734\n",
      "epoch 34500, loss 0.25721174478530884\n",
      "epoch 35000, loss 0.2571164071559906\n",
      "epoch 35500, loss 0.25693875551223755\n",
      "epoch 36000, loss 0.256859689950943\n",
      "epoch 36500, loss 0.2567855417728424\n",
      "epoch 37000, loss 0.2567090690135956\n",
      "epoch 37500, loss 0.256633996963501\n",
      "epoch 38000, loss 0.25655895471572876\n",
      "epoch 38500, loss 0.2564859390258789\n",
      "epoch 39000, loss 0.25641119480133057\n",
      "epoch 39500, loss 0.2563368082046509\n",
      "epoch 40000, loss 0.2562617361545563\n",
      "epoch 40500, loss 0.25618645548820496\n",
      "epoch 41000, loss 0.25610047578811646\n",
      "epoch 41500, loss 0.2559756338596344\n",
      "epoch 42000, loss 0.25585269927978516\n",
      "epoch 42500, loss 0.25571775436401367\n",
      "epoch 43000, loss 0.2555466294288635\n",
      "epoch 43500, loss 0.25537604093551636\n",
      "epoch 44000, loss 0.25521811842918396\n",
      "epoch 44500, loss 0.2550514340400696\n",
      "epoch 45000, loss 0.2548820674419403\n",
      "epoch 45500, loss 0.2547053098678589\n",
      "epoch 46000, loss 0.25452131032943726\n",
      "epoch 46500, loss 0.2543364465236664\n",
      "epoch 47000, loss 0.2541824281215668\n",
      "epoch 47500, loss 0.2541871666908264\n",
      "epoch 48000, loss 0.25556397438049316\n",
      "epoch 48500, loss 0.25607895851135254\n",
      "epoch 49000, loss 0.25602370500564575\n",
      "epoch 49500, loss 0.25596240162849426\n",
      "epoch 50000, loss 0.2560064196586609\n",
      "epoch 50500, loss 0.2558090388774872\n",
      "epoch 51000, loss 0.25600963830947876\n",
      "epoch 51500, loss 0.25598952174186707\n",
      "epoch 52000, loss 0.25589609146118164\n",
      "epoch 52500, loss 0.2559414505958557\n",
      "epoch 53000, loss 0.25582826137542725\n",
      "epoch 53500, loss 0.25580617785453796\n",
      "epoch 54000, loss 0.25574028491973877\n",
      "epoch 54500, loss 0.25566959381103516\n",
      "epoch 55000, loss 0.2556461989879608\n",
      "epoch 55500, loss 0.25550004839897156\n",
      "epoch 56000, loss 0.2554767429828644\n",
      "epoch 56500, loss 0.2554212212562561\n",
      "epoch 57000, loss 0.2553030550479889\n",
      "epoch 57500, loss 0.2553175389766693\n",
      "epoch 58000, loss 0.25515812635421753\n",
      "epoch 58500, loss 0.25514984130859375\n",
      "epoch 59000, loss 0.2550681531429291\n",
      "epoch 59500, loss 0.2548486292362213\n",
      "epoch 60000, loss 0.2548685371875763\n",
      "epoch 60500, loss 0.25490525364875793\n",
      "epoch 61000, loss 0.25481322407722473\n",
      "epoch 61500, loss 0.2548947036266327\n",
      "epoch 62000, loss 0.2546626925468445\n",
      "epoch 62500, loss 0.25462472438812256\n",
      "epoch 63000, loss 0.25545939803123474\n",
      "epoch 63500, loss 0.2538546323776245\n",
      "epoch 64000, loss 0.2549956738948822\n",
      "epoch 64500, loss 0.25325581431388855\n",
      "epoch 65000, loss 0.253014475107193\n",
      "epoch 65500, loss 0.2540358304977417\n",
      "epoch 66000, loss 0.25370997190475464\n",
      "epoch 66500, loss 0.2538546025753021\n",
      "epoch 67000, loss 0.25383201241493225\n",
      "epoch 67500, loss 0.25379106402397156\n",
      "epoch 68000, loss 0.2536853551864624\n",
      "epoch 68500, loss 0.2535492479801178\n",
      "epoch 69000, loss 0.25345227122306824\n",
      "epoch 69500, loss 0.25340735912323\n",
      "epoch 70000, loss 0.25346386432647705\n",
      "epoch 70500, loss 0.2532936930656433\n",
      "epoch 71000, loss 0.25326594710350037\n",
      "epoch 71500, loss 0.2532443106174469\n",
      "epoch 72000, loss 0.25298357009887695\n",
      "epoch 72500, loss 0.2530147135257721\n",
      "epoch 73000, loss 0.2530243694782257\n",
      "epoch 73500, loss 0.25273457169532776\n",
      "epoch 74000, loss 0.2528046667575836\n",
      "epoch 74500, loss 0.252692848443985\n",
      "epoch 75000, loss 0.25265005230903625\n",
      "epoch 75500, loss 0.25259724259376526\n",
      "epoch 76000, loss 0.2524890601634979\n",
      "epoch 76500, loss 0.2526322603225708\n",
      "epoch 77000, loss 0.2525980770587921\n",
      "epoch 77500, loss 0.25207749009132385\n",
      "epoch 78000, loss 0.25210168957710266\n",
      "epoch 78500, loss 0.2520343065261841\n",
      "epoch 79000, loss 0.2519501745700836\n",
      "epoch 79500, loss 0.25177299976348877\n",
      "epoch 80000, loss 0.25183242559432983\n",
      "epoch 80500, loss 0.2519221007823944\n",
      "epoch 81000, loss 0.25161272287368774\n",
      "epoch 81500, loss 0.251493901014328\n",
      "epoch 82000, loss 0.2516360282897949\n",
      "epoch 82500, loss 0.25159791111946106\n",
      "epoch 83000, loss 0.25127458572387695\n",
      "epoch 83500, loss 0.25123411417007446\n",
      "epoch 84000, loss 0.2513127028942108\n",
      "epoch 84500, loss 0.25117236375808716\n",
      "epoch 85000, loss 0.25104883313179016\n",
      "epoch 85500, loss 0.2514092028141022\n",
      "epoch 86000, loss 0.2514295279979706\n",
      "epoch 86500, loss 0.25050970911979675\n",
      "epoch 87000, loss 0.2499234825372696\n",
      "epoch 87500, loss 0.2504648268222809\n",
      "epoch 88000, loss 0.25193530321121216\n",
      "epoch 88500, loss 0.2503562569618225\n",
      "epoch 89000, loss 0.24930144846439362\n",
      "epoch 89500, loss 0.24998953938484192\n",
      "epoch 90000, loss 0.25210753083229065\n",
      "epoch 90500, loss 0.24973534047603607\n",
      "epoch 91000, loss 0.25181978940963745\n",
      "epoch 91500, loss 0.24981820583343506\n",
      "epoch 92000, loss 0.24974139034748077\n",
      "epoch 92500, loss 0.2521063983440399\n",
      "epoch 93000, loss 0.24830031394958496\n",
      "epoch 93500, loss 0.25103771686553955\n",
      "epoch 94000, loss 0.2520267069339752\n",
      "epoch 94500, loss 0.24834036827087402\n",
      "epoch 95000, loss 0.25088047981262207\n",
      "epoch 95500, loss 0.2500130534172058\n",
      "epoch 96000, loss 0.2501300573348999\n",
      "epoch 96500, loss 0.24979785084724426\n",
      "epoch 97000, loss 0.24949538707733154\n",
      "epoch 97500, loss 0.24987871944904327\n",
      "epoch 98000, loss 0.2495919167995453\n",
      "epoch 98500, loss 0.24956141412258148\n",
      "epoch 99000, loss 0.24940648674964905\n",
      "epoch 99500, loss 0.24938952922821045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_l = []\n",
    "for num_epoch in range(nb_epoch):\n",
    "  \n",
    "    if student:\n",
    "        # --- START CODE HERE (09)\n",
    "        hat_y = my_model(X) # Forward pass: Compute predicted y by passing  x to the model          \n",
    "        loss = criterion(hat_y,y)# Compute loss \n",
    "                          # Zero gradients, perform a backward pass, and update the weights. \n",
    "        optimizer.zero_grad() # re-init the gradients (otherwise they are cumulated)\n",
    "        loss.backward() # perform back-propagation\n",
    "        optimizer.step() # update the weights\n",
    "        # --- END CODE HERE\n",
    "        \n",
    "    loss_l.append(loss.detach().numpy())\n",
    "\n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, loss.item()))\n",
    "        \n",
    "# ----------------\n",
    "plt.plot(loss_l);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P2hJYgWo606"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for \n",
    "- 1) Model 1: Initialization of W1, b1, ... (01)\n",
    "- 2) Model 1: Forward-pass (02)\n",
    "- 3) Model 1: Loss and Cost computation  (03)\n",
    "- 4) Model 1: Manual update of the parameters (04)\n",
    "- 5) Model 2: using nn.sequential (05)\n",
    "- 6) Model 3: using class definition: __init__ method (06)\n",
    "- 7) Model 3: using class definition: forward method (07)\n",
    "- 8) Model 2 and 3: Loss (criterion) and parameter update algorithms (optimizer) (08)\n",
    "- 9) Model 2 and 3: code inside the loop (09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IA306_20192010_Lab_MLP_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
